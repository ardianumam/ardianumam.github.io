<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Regression Using Least Square Estimation | Ardian Umam </title> <meta name="author" content="Ardian Umam"> <meta name="description" content="Personal page of Ardian Umam "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ardianumam.github.io/blog/2017/regression-lse/"> <script src="/assets/js/theme.js?da223b2b423d03c8b9950d3e6131194c"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ardian</span> Umam's page </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Blog </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">All blogs</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/ml-note-nycu/">Machine learning note NYCU</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/my-book/">My Books </a> </li> </ul> <ul class="navbar-nav ml-auto flex-nowrap"> <div class="navbar-brand social text-right"> <a href="mailto:%61%72%64%69%61%6E%75%6D%61%6D@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=S3W6Q8sAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/ardianumam" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ardian-umam" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/ardianumam" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://instagram.com/ardianumam" title="Instagram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-instagram"></i></a> <a href="https://youtube.com/@ardianumam" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="theme-container"> <button id="theme-button" title="Change theme"> <i class="fas fa-lightbulb"></i> </button> <div class="theme-options"> <button id="light-button"> <i class="ti ti-sun-filled" id="light-icon"></i> (light) </button> <button id="dark-button"> <i class="ti ti-moon-filled" id="dark-icon"></i> (dark) </button> </div> </div> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Regression Using Least Square Estimation</h1> <p class="post-meta"> September 20, 2017 </p> <p class="post-tags"> <a href="/blog/2017"> <i class="fa-solid fa-calendar fa-sm"></i> 2017 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/regression"> <i class="fa-solid fa-hashtag fa-sm"></i> regression</a>   <a href="/blog/tag/lse"> <i class="fa-solid fa-hashtag fa-sm"></i> lse</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong><a name="bahasa"></a>Language : <a href="#english">[English]</a><a href="#bahasa">[Bahasa Indonesia]</a></strong></p> <p>Mempelajari <em>linear regression</em> adalah langkah yang baik untuk mengawali mempelajari <em>machine learning, </em>karena sederhana dan dapat memberikan intuisi bagaimana <em>machine </em>belajar dari suatu data. Lihat gambar di bawah ini.</p> <p><a href="https://ardianumam.files.wordpress.com/2017/09/regression-linear.png" rel="external nofollow noopener" target="_blank"><img class="aligncenter wp-image-120" src="https://ardianumam.files.wordpress.com/2017/09/regression-linear.png" alt="" width="246" height="197"></a></p> <p>Diberikan sejumlah data <span style="color: #ff0000;">(titik-titik warna merah), <span style="color: #000000;">dan kita ingin mendapatkan suatu fungsi garis <span style="color: #0000ff;">(garis biru)</span> yang paling sesuai untuk merepresentasikan data titik-titik tersebut. Dalam konteks <em>machine learning, </em>kita akan menggunakan data titik-titik tersebut sebagai data training untuk membuat suatu fungsi linear yang paling sesuai untuk merepresentasikan data tersebut. Gambar di atas memiliki input satu komponen nilai (satu titik) dengan output satu komponen nilai juga. Dalam case ini, kita akan menggeneralisasinya menggunakan <em>1-dimensional</em> array (vektor) dengan \(n\) komponen untuk inputnya, dengan output 1 komponen nilai. Kita dapat menuliskan model persamaan <em>linear regression</em> kita sebagai berikut, dengan \(h(\mathbf{x})\) adalah <em>hypothesis/prediction</em> untuk input \(\mathbf{x}\).</span></span></p> \[h(\mathbf{x})=a_0x_0+a_1x_1+a_2x_2+....+a_nx_n\] <p>Mungkin ada yang bertanya, kenapa linear modelnya bukan \(h(x)=ax+b\) ? Untuk bentuk tersebut saya bahas sekalian dengan bentuk polinomial <a href="https://ardianumam.wordpress.com/2017/09/21/polynomial-regression-using-least-square-estimation" rel="external nofollow noopener" target="_blank">di sini</a>, yakni dengan mensetting orde \(n=1\).</p> <p>Kembali lagi, kita dapat menuliskan persamaan di atas menggunakan notasi matrik sebagai berikut.</p> \[h(\mathbf{x})=\mathbf{a}^{T}\mathbf{x} \text{ dengan } \mathbf{a}=\begin{bmatrix} a_0\\a_1\\a_2\\... \\a_n\end{bmatrix} \text{ dan } \mathbf{x}=\begin{bmatrix} x_0\\x_1\\x_2\\... \\x_n\end{bmatrix}\] <p>Kemudian kita dapat menuliskan untuk \(m\) pasang input-output prediksi dalam notasi matrik sebagai berikut.</p> \[\begin{bmatrix} h(x_1)\\h(x_2)\\h(x_3)\\... \\h(x_m)\end{bmatrix}= \begin{bmatrix} x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\begin{bmatrix} a_0\\a_1\\a_2\\... \\a_n\end{bmatrix}\] \[h(\mathbf{x})=\mathbf{Xa}\] <p>Dalam <em>case</em> ini, <em>“design matrix” </em>kita adalah \(\begin{bmatrix} x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\), dan kita menyimbolkannya dengan \(\mathbf{X}\).</p> <p>Pertanyaan selanjutnya, bagaimana cara mengukur seberapa baik fungsi <em>linear regression </em>kita? Kita dapat membuat <em>cost function, \(J(\mathbf{a})\), </em>untuk mengukur seberapa besar eror dari fungsi <em>linear regression </em>kita. Pada case ini, kita akan menggunakan <i>average of square error</i> (rerata eror kuadrat)<i> </i> untuk <em>cost function</em> kita. Dan tujuan kita adalah meminimalkan nilai eror tersebut. Oleh karena itu, metode ini disebut dengan <em>least square estimation</em>.</p> \[J(\mathbf{a}) =\frac{1}{2m}\sum_{i=1}^{m}(h(x_i)-y_i)^{2}\] <p>Konstanta \(\frac{1}{2}\) pada persamaan di atas biasanya ditambahkan karena itu dapat menghilangkan konstanta \(2\) dari operasi kuadrat ketika kita menurunkan persamaan tersebut ke turunan pertama. Tetapi untuk kasus kita kali ini, konstanta \(\frac{1}{2m}\) akan hilang, karena kita akan membandingkan turunan pertama dari persamaan tersebut sama dengan nol. Kembali ke kasus kita, selanjutnya kita akan mensubtitusikan persamaan \(h(\mathbf{x})\)  ke  \(J(\mathbf{a})\), dan menyederhanakannya sebagai berikut.</p> \[J(\mathbf{a}) =\frac{1}{2m}(\mathbf{Xa}-\mathbf{y})^{2}=\frac{1}{2m}(\mathbf{Xa}-\mathbf{y})^T(\mathbf{Xa}-\mathbf{y})\] \[J(\mathbf{a}) =\frac{1}{2m}((\mathbf{Xa})^T-\mathbf{y}^T)(\mathbf{Xa}-\mathbf{y})\] \[J(\mathbf{a})=\frac{1}{2m}((\mathbf{Xa})^T\mathbf{Xa}-(\mathbf{Xa})^T\mathbf{y}-\mathbf{y}^T\mathbf{Xa}+\mathbf{y}^T\mathbf{y})\] <p>Simak perlahan persamaan di atas, setiap suku persamaan di atas akan menghasilkan nilai skalar. Sebagai contohnya suku ke-4 sebagai berikut:</p> \[\mathbf{y}^T\mathbf{y}=\begin{bmatrix} y_0\,y_1\,y_2\,... \,y_n\end{bmatrix}\begin{bmatrix} y_0\\y_1\\y_2\\... \\y_n\end{bmatrix}=y_0^2+y_1^2+y_2^2+... +y_n^2=\text{nilai skalar}\] <p>Sehingga, kita dapat mengubah suku ketiga, \(\mathbf{y}^T(\mathbf{Xa})=(\mathbf{Xa})^T\mathbf{y}\)  dikarenakan keduanya akan menghasilkan nilai skalar yang sama (perkalian bersifat komutatif). Kemudian, dengan mengkombinasikan suku kedua dan ketiga, kita dapat menyederhanakan persamaan \(J(\mathbf{a})\) menjadi:</p> \[J(\mathbf{a})=\frac{1}{2m}((\mathbf{Xa})^T\mathbf{Xa}-2\mathbf{Xa}^T\mathbf{y}+\mathbf{y}^T\mathbf{y})\] <p>Tujuan kita dalam <em>linear regression </em>ini sekali lagi adalah untuk mencari fungsi linear yang memiliki nilai eror, \(J(\mathbf{a})\), seminimal mungkin. Dan dalam kasus ini, kita akan mencari parameter \(\mathbf{a}\) yang memiliki nilai eror, \(J(\mathbf{a})\), paling kecil. Untuk melakukannya, kita sudah tahu dari pelajaran matematika saat SMA atau mata kuliah kalkulus di universitas yang mana dengan menghitung turunan pertama terhadap \(\mathbf{a}\), kemudian di-sama-dengankan nol. Berikut ini adalah prosesnya.</p> \[\frac{dJ(\mathbf{a})}{d\mathbf{a}}=\frac{1}{2m}(2\mathbf{X}^T\mathbf{Xa}-2\mathbf{X}^T\mathbf{y})=0\] \[(\mathbf{X}^T\mathbf{Xa}-\mathbf{X}^T\mathbf{y})=0\] \[\mathbf{X}^T\mathbf{Xa}=\mathbf{X}^T\mathbf{y}\] \[\mathbf{a}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\] <p><em><strong>Selamat!</strong> </em>Kita telah menemukan nilai parameter \(\mathbf{a}\) untuk fungsi <em>linear regression </em>kita yang paling bagus (paling kecil erornya), dengan catatan matrik \((\mathbf{X}^T\mathbf{X})\) bisa diinverskan (<em>inversible</em>). Dengan menuliskan ulang model <em>linear regression </em>kita, fungsi <em>hypothesis</em> kita menjadi:</p> \[h(\mathbf{x})=a_0x_0+a_1x_1+a_2x_2+....+a_nx_n\] <p>\(\text{dengan }\mathbf{a}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\text{, di mana }\mathbf{X}=\begin{bmatrix}x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\)  </p> <p><br></p> <hr> <p><strong><a name="english"></a>Language : <a href="#english">[English]</a><a href="#bahasa">[Bahasa Indonesia]</a></strong></p> <h2>Linear Regression using Least Square Estimation</h2> <p>It is a good start to learn regression linear on machine learning study, since it is simple and can give intuitive meaning how a machine learns a bunch of data. See picture below.</p> <p><a href="https://ardianumam.files.wordpress.com/2017/09/regression-linear.png" rel="external nofollow noopener" target="_blank"><img class="aligncenter wp-image-120" src="https://ardianumam.files.wordpress.com/2017/09/regression-linear.png" alt="" width="246" height="197"></a></p> <p>Given data points <span style="color: #ff0000;">(red dots)</span> and we want to plot a linear line that fits those data <span style="color: #0000ff;">(blue line)</span>. In machine learning context, we will use those data points as training data to make best linear function to fit those those points. Picture above has one input element with one output element. We will use 1-dimensional array with \(n\) component for input with one output element. Our linear model can be written as follow, with \(h(\mathbf{x})\) means <em>hypothesis/prediction</em> of input \(\mathbf{x}\).</p> \[h(\mathbf{x})=a_0x_0+a_1x_1+a_2x_2+....+a_nx_n\] <p>Maybe someone will ask, “<em>why do our linear model is not</em>  \(h(x)=ax+b\) <em>?</em>” For that model, I discuss it in general polynomial model <a href="https://ardianumam.wordpress.com/2017/09/21/polynomial-regression-using-least-square-estimation" rel="external nofollow noopener" target="_blank">here</a>, by setting order \(n=1\).</p> <p>Back to our case, we can represent equation above using matrix notation.</p> <p>\(h(\mathbf{x})=\mathbf{a}^{T}\mathbf{x} \)  with  \(\mathbf{a}=\begin{bmatrix} a_0\\a_1\\a_2\\... \\a_n\end{bmatrix}\)  and  \(\mathbf{x}=\begin{bmatrix} x_0\\x_1\\x_2\\... \\x_n\end{bmatrix}\)</p> <p>Then, we can write equation above in another matrix form with \(m\) pair input-output prediction.</p> \[\begin{bmatrix} h(x_1)\\h(x_2)\\h(x_3)\\... \\h(x_m)\end{bmatrix}= \begin{bmatrix} x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\begin{bmatrix} a_0\\a_1\\a_2\\... \\a_n\end{bmatrix}\] \[h(\mathbf{x})=\mathbf{Xa}\] <p>In this case,  our <em>“design matrix” </em>is \(\begin{bmatrix} x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\), and we denote it using \(\mathbf{X}\).</p> <p>The next question is, “<em>how can we measure how good our linear function toward those data points?”</em> We can define an error <em>cost function,</em> \(J(\mathbf{a})\), and we will use <em>average of square error</em> as our basic form. Our purpose is to minimize the error as small as we can. That’s why we call this method <em>least square estimation</em>.</p> \[J(\mathbf{a}) =\frac{1}{2m}\sum_{i=1}^{m}(h(x_i)-y_i)^{2}\] <p>Constant \(\frac{1}{2}\) in equation above is usually put because it can cancel constant \(2\) out from <em>square </em>operation when we take first differential. But it our case, constant \(\frac{1}{2m}\) will be thrown since later, we will compare the first differential of the equation with zero. Back to our case, we proceed to put \(h(\mathbf{x})\)  to  \(J(\mathbf{a})\) following by doing some linear algebras to simplify it. Here we go.</p> \[J(\mathbf{a}) =\frac{1}{2m}(\mathbf{Xa}-\mathbf{y})^{2}=\frac{1}{2m}(\mathbf{Xa}-\mathbf{y})^T(\mathbf{Xa}-\mathbf{y})\] \[J(\mathbf{a}) =\frac{1}{2m}((\mathbf{Xa})^T-\mathbf{y}^T)(\mathbf{Xa}-\mathbf{y})\] \[J(\mathbf{a})=\frac{1}{2m}((\mathbf{Xa})^T\mathbf{Xa}-(\mathbf{Xa})^T\mathbf{y}-\mathbf{y}^T\mathbf{Xa}+\mathbf{y}^T\mathbf{y})\] <p>Look carefully to the equation above, each term above will produce scalar value, for example:</p> \[\mathbf{y}^T\mathbf{y}=\begin{bmatrix} y_0\,y_1\,y_2\,... \,y_n\end{bmatrix}\begin{bmatrix} y_0\\y_1\\y_2\\... \\y_n\end{bmatrix}=y_0^2+y_1^2+y_2^2+...+y_n^2=scalar\,value\] <p>Thus, we can change third term, \(\mathbf{y}^T(\mathbf{Xa})=(\mathbf{Xa})^T\mathbf{y}\)  since these will produce same scalar value (multiplication is commutative). Then, by combining second and third term, we can simplify \(J(\mathbf{a})\) become:</p> \[J(\mathbf{a})=\frac{1}{2m}((\mathbf{Xa})^T\mathbf{Xa}-2\mathbf{Xa}^T\mathbf{y}+\mathbf{y}^T\mathbf{y})\] <p>Our purpose for linear regression is to find a linear function that gives minimal error value, \(J(\mathbf{a})\). And in our case, we will find parameter \(\mathbf{a}\) that gives minimal value of \(J(\mathbf{a})\). To do so, we already know in high school math or in undergraduate calculus course that we can take first differential toward \(\mathbf{a}\), and make it equal to zero. Here we go.</p> \[\frac{dJ(\mathbf{a})}{d\mathbf{a}}=\frac{1}{2m}(2\mathbf{X}^T\mathbf{Xa}-2\mathbf{X}^T\mathbf{y})=0\] \[(\mathbf{X}^T\mathbf{Xa}-\mathbf{X}^T\mathbf{y})=0\] \[\mathbf{X}^T\mathbf{Xa}=\mathbf{X}^T\mathbf{y}\] \[\mathbf{a}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\] <p><em><strong>Voila!</strong></em> We can get the value of \(\mathbf{a}\) for our linear function that best fits our data points, assuming that  \((\mathbf{X}^T\mathbf{X})\) is invertible. Thus, re-plugging in to our linear model, our <em>hypothesis</em> function become:</p> \[h(\mathbf{x})=a_0x_0+a_1x_1+a_2x_2+....+a_nx_n\] \[\text{with }\mathbf{a}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \text{, where }\mathbf{X}=\begin{bmatrix} x_{10}\,\,x_{11}\,\,x_{12}\,... \,x_{1n}\\x_{20}\,\,x_{21}\,\,x_{22}\,... \,x_{2n}\\x_{30}\,\,x_{31}\,\,x_{32}\,... \,x_{3n}\\... \,\,... \,\,... \,... \\x_{m0}\,\,x_{m1}\,\,x_{m2}\,... \,x_{mn}\end{bmatrix}\] </div> </article> <div id="disqus_thread" style="max-width: 950px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="ardianumam-github-io",disqus_identifier="/blog/2017/regression-lse",disqus_title="Linear Regression Using\xa0Least Square Estimation";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ardian Umam. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: 2024 November 11. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>